{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype of model explanation via LIME with help of extractive summary\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import time\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime(\"%Y-%m-%d_%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-3-d59e77e64a5c>:14} CRITICAL - Logging LIME with new TF model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, \n",
    "    format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(filename=f'../../data/logs/v1-{now}.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "l = logging.getLogger('prototype')\n",
    "l.critical(\"Logging LIME with new TF model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-related\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Define a function that loads a model to be explained and returns its instance\n",
    "    \"\"\"\n",
    "    \n",
    "    return keras.models.load_model(\"../../raw-data/lstm-model-sigmoid\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-5-b2e056f55d37>:2} INFO - Model loaded\n"
     ]
    }
   ],
   "source": [
    "model = load_model()\n",
    "l.info(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0706619]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"hahahahahahahahahaha this is the most boring film I have ever seen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-598b0595b113>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "[{deprecation.py:317} WARNING - From <ipython-input-7-598b0595b113>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.948632]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba([\"hahahahahahahahahaha this is the funniest film I have ever seen\"])\n",
    "# Even though model has function `predict_proba`, it is not sufficient for LIME\n",
    "# LIME expects this predict_proba function to return probability for each of the predicted classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "import os\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/home/tomasmizera/school/diploma/src/data/reviews\"\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 6\n",
    "TOP_FEATURES_COUNT = 10 \n",
    "\n",
    "REVIEW_IX = 0\n",
    "EXPL_IX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = TextRankSummarizer(Stemmer(LANGUAGE))\n",
    "summarizer.stop_words = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanator = lime_text.LimeTextExplainer(class_names=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-12-9e84a1d64b6b>:1} INFO - Starting an algorithm\n"
     ]
    }
   ],
   "source": [
    "l.info(\"Starting an algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a decorator to log execusion time\n",
    "# inspired by https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        timed.calls += 1\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        timed.time_taken += (te - ts) * 1000\n",
    "        return result\n",
    "    timed.calls = 0\n",
    "    timed.time_taken = 0\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timeit # for LIME it is called once and for this model takes around 150ms with logging\n",
    "def _predict_proba(_input):\n",
    "    \"\"\"\n",
    "    Define a function that accepts array of instances and returns a probability for each class \n",
    "    _input - 1d array of instances\n",
    "    Returns 2d array of [num of instances] x [num of classes] with probabilities\n",
    "    \"\"\"\n",
    "    prediction = model.predict( _input )\n",
    "    \n",
    "    return np.append(prediction, 1 - prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _explain_instance(_file, _explanator):\n",
    "    explanation = _explanator.explain_instance(_file, _predict_proba, num_features=TOP_FEATURES_COUNT)\n",
    "#     l.info('_predict_proba took:  %2.2f ms' % \\\n",
    "#                   (_predict_proba.time_taken))\n",
    "#     l.info(f'_predict_proba called {_predict_proba.calls} times')\n",
    "#     _predict_proba.calls = 0\n",
    "#     _predict_proba.time_taken = 0\n",
    "    \n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some examples of perturbed text**: \\[ ..., ' great hm', ' great ', ' great hm', '  hm', '  ', '  hm', '  ', '  ', '  ', '  ', 'not great ', '  ', '  hm', '  ', '  ', 'not great ', '  ', ' great ', ' great ', '  hm', '  ', ' great ', 'not  hm', ' great hm', ' great ', ' great hm', '  hm', 'not  ', '  hm', 'not  ', ' great hm', 'not great ', ' great ', '  ', '  ', '  hm', '  ', '  ', '  ', 'not  hm', 'not  ', 'not great ', 'not  hm', 'not  hm', 'not great ', 'not  hm', '  ', ' great ', '  ', '  hm', 'not  hm', '  ', '  ', ' great ', '  ', '  ', ' great ', 'not  ', 'not  hm', ' great ', 'not  ', 'not  ', 'not  hm', 'not  ', '  ', 'not great ', '  hm', ' great hm', '  hm', '  ', '  ', 'not  hm', '  hm', 'not  ', '  ', ' great hm', 'not  ', ' great hm', 'not  hm', 'not  ', 'not  hm', '  ', ' great ', '  hm', ' great ', 'not  hm', 'not  ', 'not  ', ' great hm', 'not  hm', '  hm', '  hm', ' great ', '  ', 'not great ', '  hm', 'not great ', '  ', '  ', 'not  hm', 'not great ', '  ', '  ', ' great hm', 'not  hm', 'not  hm', ' great ', ' great hm', '  ', 'not  hm', ' great ', '  hm', ' great ', 'not great ', '  ', '  ', ' great hm', ' great hm', '  ', ' great ', 'not  hm', ' great ', 'not  ', ' great ', 'not great ', ' great ', 'not  ', 'not great ', '  ', '  ', '  ', 'not  ', ' great hm', ' great hm', '  ', '  ', 'not great ', '  ', '  ', 'not  ', ' great ', ' great ', 'not great ', '  ', '  ', '  ', '  ', '  ', '  ', '  hm', '  hm', 'not  ', 'not  hm', 'not  ', '  ', ' great ', '  hm', ' great hm', '  ', '  ', '  ', ' great hm', '  ', '  ', 'not  hm', '  ', ' great hm', ' great hm', ' great ', '  ', '  ', 'not  ', '  ', 'not  ', ' great hm', 'not great ', ' great hm', 'not  hm', 'not great ', 'not  ', 'not great ', '  ', 'not great ', '  hm', 'not  ', ' great ', '  ', '  ', ' great ', '  hm', 'not  ', 'not  ', 'not  ', '  ', ' great hm', ' great hm', ' great ', ' great ', 'not great ', ' great ', '  ', 'not great ', 'not great ', '  ', 'not  ', ' great ', ' great ', '  ', '  ', ' great ', 'not  ', ' great ', '  ', '  ', ' great ', 'not  hm', 'not  ', '  ', 'not great ', '  ', '  ', '  ', 'not great ', 'not great ', '  ', '  hm', '  hm', 'not  hm', 'not great ', ' great ', 'not  ', '  ', 'not  hm', 'not great ', 'not  ', 'not great ', 'not  hm', 'not  ', 'not  hm', ' great hm', ' great ', '  hm', '  ', '  hm', '  ', 'not  ', ' great ', '  ', '  hm', 'not  hm', 'not great ', '  ', '  ', ' great ', '  ', '  ', 'not  ', 'not  ', ' great ', '  ', 'not  hm', '  ', ' great hm', '  ', '  ', '  ', ' great ', ' great ', ' great ', ' great ', ' great ', '  hm', 'not  ', ' great ', 'not  hm', '  ', ' great hm', 'not great ', '  hm', '  hm', 'not  hm', 'not  ', 'not great ', '  ', '  ', '  ', 'not  ', '  ', ' great ', '  ', 'not great ', '  ', 'not  ', '  ', '  hm', ' great ', '  ', 'not  ', ' great hm', '  ', ' great ', 'not great ', '  ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_from_files(path_to_files):\n",
    "    \"\"\"\n",
    "    Loads all readable files in path_to_files directory\n",
    "    Returns np.array with each files content as a separate element\n",
    "    \"\"\"\n",
    "    \n",
    "    def _read_text_file(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            return reduce(lambda a, b: a + b, f.readlines())\n",
    "    \n",
    "    files_it = os.scandir(path_to_files)\n",
    "    files_contents = {}\n",
    "    \n",
    "    for file in files_it:\n",
    "        if file.is_file(): \n",
    "            files_contents[file.name] = _read_text_file(file.path)\n",
    "        \n",
    "    return files_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "npInput = input_from_files(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summarize_doc_custom(_summarizer, _instance, _explanation):\n",
    "    \"\"\"\n",
    "    Returns summary with altered weights based on explanation\n",
    "    _summarizer - summy summarizer instance\n",
    "    _instance - instance content string\n",
    "    _explanation - LIME explanation\n",
    "    \"\"\"\n",
    "    \n",
    "    def _create_weight_graph(_summarizer, _instance_doc):\n",
    "        return _summarizer.rate_sentences(_instance_doc)\n",
    "    \n",
    "    def _count_factor(_sentence, _explanation_words_weight) -> float: # returns boosting factor for sentence\n",
    "        factor = 1.0\n",
    "        exp_words = list(map(lambda x: x[0], _explanation_words_weight))\n",
    "        for word in _sentence.words:\n",
    "            if word in exp_words:\n",
    "                factor += abs(_explanation_words_weight[exp_words.index(word)][1])        \n",
    "        return factor, 10 ** factor # factor * 3 if factor != 1 else factor # TODO: Parameter tuning for factor scale\n",
    "    \n",
    "    parser = PlaintextParser.from_string(_instance, Tokenizer(LANGUAGE))\n",
    "    graph = _create_weight_graph(_summarizer, parser.document)\n",
    "    \n",
    "    for sentence in graph.keys():\n",
    "        basic, factor = _count_factor(sentence, _explanation.as_list())\n",
    "#         l.info(\"IX: Sentence \" + str(sentence)[:20] + \" previous weight: \" + str(graph[sentence]) + \" bf: \" + str(basic) + \" cf: \" + str(factor) + \" new weight: \" + str(graph[sentence] * factor))\n",
    "        graph[sentence] = graph[sentence] * factor \n",
    "        \n",
    "    resulting_summary = _summarizer._get_best_sentences(parser.document.sentences, SENTENCES_COUNT, graph)\n",
    "    \n",
    "    return resulting_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summary_to_string(_summary):\n",
    "    if len(_summary) <= 0:\n",
    "        return \"\"\n",
    "    \n",
    "    summary_str = str(_summary[0])\n",
    "    i = 1\n",
    "    \n",
    "    while(i < len(_summary)):\n",
    "        summary_str += ' ' + str(_summary[i])\n",
    "        i += 1\n",
    "        \n",
    "    return summary_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_explanation_summaries(_instance_map, _explanator, _summarizer):\n",
    "    \"\"\"\n",
    "    Returns summaries for all input elements\n",
    "    _instance_map - map containing instance name and its content\n",
    "    _explanator - LIME explanator instance\n",
    "    _summarizer - summy summarizer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    summaries = {}\n",
    "    \n",
    "    for instance in _instance_map.keys():\n",
    "        explanation = _explain_instance(_instance_map[instance], _explanator)\n",
    "        summary = _summarize_doc_custom(_summarizer, _instance_map[instance], explanation)\n",
    "        summaries[instance] = (_summary_to_string(summary), explanation.as_list())\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_summaries(_instance_map, _summarizer):\n",
    "    \"\"\"\n",
    "    Returns summaries for all input instances\n",
    "    _instance_map - map containing instance name and its content\n",
    "    _summarizer - summy summarizer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    summaries = {}\n",
    "    \n",
    "    for instance in _instance_map.keys():\n",
    "        _parser = PlaintextParser.from_string(_instance_map[instance], Tokenizer(LANGUAGE))\n",
    "        summaries[instance] = _summary_to_string(_summarizer(_parser.document, SENTENCES_COUNT))\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_sums = create_explanation_summaries(npInput, explanator, summarizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_sums = create_simple_summaries(npInput, summarizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary visualization\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_summary(_summary):\n",
    "    \"\"\"\n",
    "    TBD: all possible visualization options\n",
    "    TODO: choose correct colors\n",
    "    \"\"\"\n",
    "    colors = {}\n",
    "    colors['blue'] = '54,151,186'\n",
    "    colors['green'] = '0,127,0'\n",
    "    alpha = 0.5\n",
    "    \n",
    "    start_highlight_tag = f'<mark style=\"background-color:rgba({colors[\"blue\"]},{alpha})\">'\n",
    "    end_highlight_tag = '</mark>'\n",
    "    \n",
    "    raw_text = _summary[0]\n",
    "    important_words_weights = _summary[1]\n",
    "    important_words = list(map(lambda x: x[0], important_words_weights))\n",
    "    \n",
    "    # TODO: maybe \n",
    "#     parsed = PlaintextParser.from_string(raw_text, Tokenizer(LANGUAGE)).document\n",
    "    \n",
    "#     result = ''\n",
    "#     for sentence in parsed.sentences:\n",
    "#         converted = _summary_to_string([sentence]) # convert parsed sentence to string\n",
    "#         for word in sentence.words:\n",
    "#             if word in important_words:\n",
    "                \n",
    "    for word in important_words:\n",
    "        # TODO: alter alpha based on word weight\n",
    "        raw_text = raw_text.replace(word, start_highlight_tag + word + end_highlight_tag)\n",
    "        \n",
    "        \n",
    "    display(HTML(raw_text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One mbbbbn’s determinbbbbtion to keep his life completely old-school proves utterly ruinous in “Hunter Hunter,” bbbb movie written bbbbnd directed by Shbbbbwn Linden.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_summary_to_string([a.document.sentences[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "One man’s determination to keep his life completely old-school proves utterly ruinous in “Hunter Hunter,” a movie written and directed by Shawn Linden. <mark style=\"background-color:rgba(54,151,186,0.5)\">Devon</mark> Sawa plays Joe <mark style=\"background-color:rgba(54,151,186,0.5)\">Mersault</mark>, a trapper whose nuclear family—wife-of-saintly-patience Anne (<mark style=\"background-color:rgba(54,151,186,0.5)\">Camille</mark> <mark style=\"background-color:rgba(54,151,186,0.5)\">Sullivan</mark>) and eager if <mark style=\"background-color:rgba(54,151,186,0.5)\">occasionally</mark> queasy <mark style=\"background-color:rgba(54,151,186,0.5)\">student</mark> of the <mark style=\"background-color:rgba(54,151,186,0.5)\">traditional</mark> <mark style=\"background-color:rgba(54,151,186,0.5)\">ways</mark> tween <mark style=\"background-color:rgba(54,151,186,0.5)\">daughter</mark> Renée (Summer H. Howell)—live off the land in a cabin in some deep Northern woods (the movie was <mark style=\"background-color:rgba(54,151,186,0.5)\">largely</mark> shot in Manitoba)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_summary(explanation_sums['review-low.txt-test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('student', 0.1756947908756967),\n",
       " ('Camille', -0.15570757009482525),\n",
       " ('largely', 0.12706037611284374),\n",
       " ('Mersault', -0.12293723867266335),\n",
       " ('traditional', -0.11894629221493297),\n",
       " ('ways', -0.11475875686357108),\n",
       " ('occasionally', -0.10254190146636212),\n",
       " ('Sullivan', 0.09509741487001556),\n",
       " ('daughter', 0.09431417561061968),\n",
       " ('Devon', 0.09342166467487582)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation_sums['review-low.txt-test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<mark style=\"background-color:rgba(54, 151, 186, 1.0)\">Hellloo!</mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "highlight_start = f'<mark style=\"background-color:rgba(54, 151, 186, {1.0})\">'\n",
    "highlight_end = '</mark>'\n",
    "text = ''.join([highlight_start, \"Hellloo!\", highlight_end])\n",
    "\n",
    "display(HTML(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_highlighted_summary(summary_pair):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [x] find a good pytorch/tf LSTM text classification model ~ maybe check datasets in LIME paper\n",
    "- [x] create predict_proba based on the type of the framework\n",
    "- [ ] predict on created summaries ~ automatically -> (possible: save summaries to files and then load and pass them just as normal instance) \n",
    "- [ ] refactor process to not store everything in RAM, rather put intermediate results to files\n",
    "- [ ] highlighting of important words from any summary (maybe save both, str summary and Sentence type summary - from sumy)\n",
    "- [ ] extract it to separate script\n",
    "- [ ] add better logging (more logs in this version)\n",
    "- [ ] build and test quantitative experiment pipeline\n",
    "- [ ] maybe find better dataset (longer texts) for data and train another model for it\n",
    "- [ ] run quantitative experiment on all instances\n",
    "- [ ] pick several (~6) explanations for user-study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
