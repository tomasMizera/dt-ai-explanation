{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype of model explanation via LIME with help of extractive summary\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import time\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime(\"%Y-%m-%d_%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-3-d59e77e64a5c>:14} CRITICAL - Logging LIME with new TF model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, \n",
    "    format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(filename=f'../../data/logs/v1-{now}.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "l = logging.getLogger('prototype')\n",
    "l.critical(\"Logging LIME with new TF model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-related\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Define a function that loads a model to be explained and returns its instance\n",
    "    \"\"\"\n",
    "    \n",
    "    return keras.models.load_model(\"../../raw-data/lstm-model-sigmoid\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-5-b2e056f55d37>:2} INFO - Model loaded\n"
     ]
    }
   ],
   "source": [
    "model = load_model()\n",
    "l.info(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0706619]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"hahahahahahahahahaha this is the most boring film I have ever seen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-598b0595b113>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "[{deprecation.py:317} WARNING - From <ipython-input-7-598b0595b113>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.948632]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba([\"hahahahahahahahahaha this is the funniest film I have ever seen\"])\n",
    "# Even though model has function `predict_proba`, it is not sufficient for LIME\n",
    "# LIME expects this predict_proba function to return probability for each of the predicted classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "import os\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/home/tomasmizera/school/diploma/src/data/reviews\"\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 6\n",
    "TOP_FEATURES_COUNT = 100\n",
    "\n",
    "REVIEW_IX = 0\n",
    "EXPL_IX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = TextRankSummarizer(Stemmer(LANGUAGE))\n",
    "summarizer.stop_words = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanator = lime_text.LimeTextExplainer(class_names=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-12-9e84a1d64b6b>:1} INFO - Starting an algorithm\n"
     ]
    }
   ],
   "source": [
    "l.info(\"Starting an algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a decorator to log execusion time\n",
    "# inspired by https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        timed.calls += 1\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        timed.time_taken += (te - ts) * 1000\n",
    "        return result\n",
    "    timed.calls = 0\n",
    "    timed.time_taken = 0\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @timeit # for LIME it is called once and for this model takes around 150ms with logging\n",
    "def _predict_proba(_input):\n",
    "    \"\"\"\n",
    "    Define a function that accepts array of instances and returns a probability for each class \n",
    "    _input - 1d array of instances\n",
    "    Returns 2d array of [num of instances] x [num of classes] with probabilities\n",
    "    \"\"\"\n",
    "    prediction = model.predict( _input )\n",
    "    \n",
    "    return np.append(prediction, 1 - prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _explain_instance(_file, _explanator):\n",
    "    explanation = _explanator.explain_instance(_file, _predict_proba, num_features=TOP_FEATURES_COUNT)\n",
    "#     l.info('_predict_proba took:  %2.2f ms' % \\\n",
    "#                   (_predict_proba.time_taken))\n",
    "#     l.info(f'_predict_proba called {_predict_proba.calls} times')\n",
    "#     _predict_proba.calls = 0\n",
    "#     _predict_proba.time_taken = 0\n",
    "    \n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some examples of perturbed text**: \\[ ..., ' great hm', ' great ', ' great hm', '  hm', '  ', '  hm', '  ', '  ', '  ', '  ', 'not great ', '  ', '  hm', '  ', '  ', 'not great ', '  ', ' great ', ' great ', '  hm', '  ', ' great ', 'not  hm', ' great hm', ' great ', ' great hm', '  hm', 'not  ', '  hm', 'not  ', ' great hm', 'not great ', ' great ', '  ', '  ', '  hm', '  ', '  ', '  ', 'not  hm', 'not  ', 'not great ', 'not  hm', 'not  hm', 'not great ', 'not  hm', '  ', ' great ', '  ', '  hm', 'not  hm', '  ', '  ', ' great ', '  ', '  ', ' great ', 'not  ', 'not  hm', ' great ', 'not  ', 'not  ', 'not  hm', 'not  ', '  ', 'not great ', '  hm', ' great hm', '  hm', '  ', '  ', 'not  hm', '  hm', 'not  ', '  ', ' great hm', 'not  ', ' great hm', 'not  hm', 'not  ', 'not  hm', '  ', ' great ', '  hm', ' great ', 'not  hm', 'not  ', 'not  ', ' great hm', 'not  hm', '  hm', '  hm', ' great ', '  ', 'not great ', '  hm', 'not great ', '  ', '  ', 'not  hm', 'not great ', '  ', '  ', ' great hm', 'not  hm', 'not  hm', ' great ', ' great hm', '  ', 'not  hm', ' great ', '  hm', ' great ', 'not great ', '  ', '  ', ' great hm', ' great hm', '  ', ' great ', 'not  hm', ' great ', 'not  ', ' great ', 'not great ', ' great ', 'not  ', 'not great ', '  ', '  ', '  ', 'not  ', ' great hm', ' great hm', '  ', '  ', 'not great ', '  ', '  ', 'not  ', ' great ', ' great ', 'not great ', '  ', '  ', '  ', '  ', '  ', '  ', '  hm', '  hm', 'not  ', 'not  hm', 'not  ', '  ', ' great ', '  hm', ' great hm', '  ', '  ', '  ', ' great hm', '  ', '  ', 'not  hm', '  ', ' great hm', ' great hm', ' great ', '  ', '  ', 'not  ', '  ', 'not  ', ' great hm', 'not great ', ' great hm', 'not  hm', 'not great ', 'not  ', 'not great ', '  ', 'not great ', '  hm', 'not  ', ' great ', '  ', '  ', ' great ', '  hm', 'not  ', 'not  ', 'not  ', '  ', ' great hm', ' great hm', ' great ', ' great ', 'not great ', ' great ', '  ', 'not great ', 'not great ', '  ', 'not  ', ' great ', ' great ', '  ', '  ', ' great ', 'not  ', ' great ', '  ', '  ', ' great ', 'not  hm', 'not  ', '  ', 'not great ', '  ', '  ', '  ', 'not great ', 'not great ', '  ', '  hm', '  hm', 'not  hm', 'not great ', ' great ', 'not  ', '  ', 'not  hm', 'not great ', 'not  ', 'not great ', 'not  hm', 'not  ', 'not  hm', ' great hm', ' great ', '  hm', '  ', '  hm', '  ', 'not  ', ' great ', '  ', '  hm', 'not  hm', 'not great ', '  ', '  ', ' great ', '  ', '  ', 'not  ', 'not  ', ' great ', '  ', 'not  hm', '  ', ' great hm', '  ', '  ', '  ', ' great ', ' great ', ' great ', ' great ', ' great ', '  hm', 'not  ', ' great ', 'not  hm', '  ', ' great hm', 'not great ', '  hm', '  hm', 'not  hm', 'not  ', 'not great ', '  ', '  ', '  ', 'not  ', '  ', ' great ', '  ', 'not great ', '  ', 'not  ', '  ', '  hm', ' great ', '  ', 'not  ', ' great hm', '  ', ' great ', 'not great ', '  ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_from_files(path_to_files):\n",
    "    \"\"\"\n",
    "    Loads all readable files in path_to_files directory\n",
    "    Returns np.array with each files content as a separate element\n",
    "    \"\"\"\n",
    "    \n",
    "    def _read_text_file(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            return reduce(lambda a, b: a + b, f.readlines())\n",
    "    \n",
    "    files_it = os.scandir(path_to_files)\n",
    "    files_contents = {}\n",
    "    \n",
    "    for file in files_it:\n",
    "        if file.is_file(): \n",
    "            files_contents[file.name] = _read_text_file(file.path)\n",
    "        \n",
    "    return files_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "npInput = input_from_files(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summarize_doc_custom(_summarizer, _instance, _explanation):\n",
    "    \"\"\"\n",
    "    Returns summary with altered weights based on explanation\n",
    "    _summarizer - summy summarizer instance\n",
    "    _instance - instance content string\n",
    "    _explanation - LIME explanation\n",
    "    \"\"\"\n",
    "    \n",
    "    def _create_weight_graph(_summarizer, _instance_doc):\n",
    "        return _summarizer.rate_sentences(_instance_doc)\n",
    "    \n",
    "    def _count_factor(_sentence, _explanation_words_weight) -> float: # returns boosting factor for sentence\n",
    "        factor = 1.0\n",
    "        exp_words = list(map(lambda x: x[0], _explanation_words_weight))\n",
    "        for word in _sentence.words:\n",
    "            if word in exp_words:\n",
    "                factor += abs(_explanation_words_weight[exp_words.index(word)][1]) # TODO: apply factor scaling rather HERE than when returning! \n",
    "        return factor, 10 ** factor # factor * 3 if factor != 1 else factor # TODO: Parameter tuning for factor scale\n",
    "    \n",
    "    parser = PlaintextParser.from_string(_instance, Tokenizer(LANGUAGE))\n",
    "    graph = _create_weight_graph(_summarizer, parser.document)\n",
    "    \n",
    "    for sentence in graph.keys():\n",
    "        basic, factor = _count_factor(sentence, _explanation.as_list())\n",
    "#         l.info(\"IX: Sentence \" + str(sentence)[:20] + \" previous weight: \" + str(graph[sentence]) + \" bf: \" + str(basic) + \" cf: \" + str(factor) + \" new weight: \" + str(graph[sentence] * factor))\n",
    "        graph[sentence] = graph[sentence] * factor \n",
    "        \n",
    "    resulting_summary = _summarizer._get_best_sentences(parser.document.sentences, SENTENCES_COUNT, graph)\n",
    "    \n",
    "    return resulting_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summary_to_string(_summary):\n",
    "    if len(_summary) <= 0:\n",
    "        return \"\"\n",
    "    \n",
    "    summary_str = str(_summary[0])\n",
    "    i = 1\n",
    "    \n",
    "    while(i < len(_summary)):\n",
    "        summary_str += ' ' + str(_summary[i])\n",
    "        i += 1\n",
    "        \n",
    "    return summary_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_explanation_summaries(_instance_map, _explanator, _summarizer):\n",
    "    \"\"\"\n",
    "    Returns summaries for all input elements\n",
    "    _instance_map - map containing instance name and its content\n",
    "    _explanator - LIME explanator instance\n",
    "    _summarizer - summy summarizer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    summaries = {}\n",
    "    \n",
    "    for instance in _instance_map.keys():\n",
    "        explanation = _explain_instance(_instance_map[instance], _explanator)\n",
    "        summary = _summarize_doc_custom(_summarizer, _instance_map[instance], explanation)\n",
    "        summaries[instance] = (_summary_to_string(summary), explanation.as_list())\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_summaries(_instance_map, _summarizer):\n",
    "    \"\"\"\n",
    "    Returns summaries for all input instances\n",
    "    _instance_map - map containing instance name and its content\n",
    "    _summarizer - summy summarizer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    summaries = {}\n",
    "    \n",
    "    for instance in _instance_map.keys():\n",
    "        _parser = PlaintextParser.from_string(_instance_map[instance], Tokenizer(LANGUAGE))\n",
    "        summaries[instance] = _summary_to_string(_summarizer(_parser.document, SENTENCES_COUNT))\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_sums = create_explanation_summaries(npInput, explanator, summarizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_sums = create_simple_summaries(npInput, summarizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary visualization\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def highlight_summary(_summary):\n",
    "    \"\"\"\n",
    "    TBD: all possible visualization options\n",
    "    TODO: choose correct colors\n",
    "    \"\"\"\n",
    "    colors = {}\n",
    "    colors['blue'] = '54,151,186'\n",
    "    colors['green'] = '0,127,0'\n",
    "    \n",
    "    start_highlight_tag = lambda col, a: f'<mark style=\"background-color:rgba({colors[col]},{a})\">'\n",
    "    end_highlight_tag = '</mark>'\n",
    "    \n",
    "    raw_text = _summary[0]\n",
    "    important_words_weights = _summary[1]\n",
    "    important_words = list(map(lambda x: x[0], important_words_weights))\n",
    "    maxv = abs(important_words_weights[0][1])\n",
    "    minv = abs(important_words_weights[-1][1]) # Here take abs for alpha calculations\n",
    "    \n",
    "    # TODO: maybe ignore words that has way too low weight?\n",
    "    \n",
    "    for ix, word in enumerate(important_words):\n",
    "        # normalize alpha to 0, 1\n",
    "        wx = important_words_weights[ix][1] \n",
    "        col = \"blue\" if wx >= 0 else \"green\"\n",
    "        alpha = (abs(wx) - minv) / (maxv - minv) # consider weights as positive due to alpha\n",
    "        pattern = r'(?<![><(=\")\\/])\\b(' + word + r')\\b(?!(:rgba)|(=\"back))'\n",
    "        # https://regex101.com/r/nNu7Rs/1\n",
    "        raw_text = re.sub(pattern, start_highlight_tag(col, alpha) + word + end_highlight_tag, raw_text, flags=re.I)\n",
    "        \n",
    "        \n",
    "    display(HTML(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Played onstage in <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> original 1984 production <mark style=\"background-color:rgba(0,127,0,1.0)\">and</mark> its <mark style=\"background-color:rgba(54,151,186,0.07388994842772252)\">2003</mark> revival (both <mark style=\"background-color:rgba(54,151,186,0.1614652323711794)\">of</mark> which <mark style=\"background-color:rgba(0,127,0,0.5109058896796954)\">I</mark> have seen) by Charles <mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark>. <mark style=\"background-color:rgba(0,127,0,0.17193005101814152)\">Dutton</mark>, Levee <mark style=\"background-color:rgba(0,127,0,0.41952857310539654)\">is</mark> <mark style=\"background-color:rgba(54,151,186,0.10264053209897611)\">only</mark> <mark style=\"background-color:rgba(0,127,0,0.3020908603287354)\">as</mark> <mark style=\"background-color:rgba(0,127,0,0.17411686049661307)\">sweet</mark> <mark style=\"background-color:rgba(0,127,0,1.0)\">and</mark> charismatic <mark style=\"background-color:rgba(0,127,0,0.3020908603287354)\">as</mark> <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> situation he <mark style=\"background-color:rgba(0,127,0,0.41952857310539654)\">is</mark> in requires. Levee <mark style=\"background-color:rgba(0,127,0,1.0)\">and</mark> Ma are <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> live wires, <mark style=\"background-color:rgba(54,151,186,0.05344565159128807)\">but</mark> <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> rest <mark style=\"background-color:rgba(54,151,186,0.1614652323711794)\">of</mark> <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> <mark style=\"background-color:rgba(0,127,0,0.24959193095026064)\">band</mark> <mark style=\"background-color:rgba(0,127,0,0.41952857310539654)\">is</mark> more pragmatic, either due <mark style=\"background-color:rgba(54,151,186,0.32049361768414014)\">to</mark> age, wisdom or merely wanting <mark style=\"background-color:rgba(54,151,186,0.32049361768414014)\">to</mark> get in <mark style=\"background-color:rgba(0,127,0,1.0)\">and</mark> <mark style=\"background-color:rgba(0,127,0,0.22243283035025802)\">out</mark> <mark style=\"background-color:rgba(0,127,0,0.3020908603287354)\">as</mark> quickly <mark style=\"background-color:rgba(0,127,0,0.3020908603287354)\">as</mark> possible. They’re <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> first three <mark style=\"background-color:rgba(54,151,186,0.32049361768414014)\">to</mark> arrive, meeting Ma’<mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark> agent Irvin (Jeremy Shamos) at <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> <mark style=\"background-color:rgba(54,151,186,0.06194102591790957)\">rather</mark> rundown recording <mark style=\"background-color:rgba(0,127,0,0.32968846322717266)\">studio</mark> where they are <mark style=\"background-color:rgba(54,151,186,0.32049361768414014)\">to</mark> record <mark style=\"background-color:rgba(54,151,186,0.030083195927260296)\">an</mark> album <mark style=\"background-color:rgba(54,151,186,0.1614652323711794)\">of</mark> Ma’<mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark> biggest numbers (and a Bessie Smith <mark style=\"background-color:rgba(54,151,186,0.05278622893618242)\">cover</mark> or two, which <mark style=\"background-color:rgba(0,127,0,0.41952857310539654)\">is</mark> sure <mark style=\"background-color:rgba(54,151,186,0.32049361768414014)\">to</mark> ruffle Ma’<mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark> feathers). <mark style=\"background-color:rgba(54,151,186,0.11592457893487075)\">Wilson</mark>’<mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark> penchant for sprinkling symbolically supernatural elements into <mark style=\"background-color:rgba(0,127,0,0.17236290813082433)\">his</mark> <mark style=\"background-color:rgba(0,127,0,0.16506006254538672)\">plays</mark> gets <mark style=\"background-color:rgba(54,151,186,0.030083195927260296)\">an</mark> amusing, ironic tinge here—<mark style=\"background-color:rgba(0,127,0,0.33435853804955773)\">it</mark> <mark style=\"background-color:rgba(54,151,186,0.09197005660840003)\">seems</mark> <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> <mark style=\"background-color:rgba(54,151,186,0.10264053209897611)\">only</mark> way for a Black man <mark style=\"background-color:rgba(54,151,186,0.32049361768414014)\">to</mark> <mark style=\"background-color:rgba(0,127,0,0.103091131282956)\">enjoy</mark> <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> <mark style=\"background-color:rgba(54,151,186,0.0040398089275313675)\">same</mark> freedom <mark style=\"background-color:rgba(0,127,0,0.3020908603287354)\">as</mark> <mark style=\"background-color:rgba(0,127,0,0.17236290813082433)\">his</mark> <mark style=\"background-color:rgba(0,127,0,0.19283189683296875)\">White</mark> counterpart in <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> 1920’<mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark> <mark style=\"background-color:rgba(0,127,0,0.41952857310539654)\">is</mark> <mark style=\"background-color:rgba(54,151,186,0.32049361768414014)\">to</mark> broker a <mark style=\"background-color:rgba(0,127,0,0.20863418567085146)\">deal</mark> with Beelzebub. One <mark style=\"background-color:rgba(54,151,186,0.1614652323711794)\">of</mark> Ma’<mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark> requirements before she records “Ma <mark style=\"background-color:rgba(54,151,186,0.19381619576406814)\">Rainey</mark>’<mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark> Black <mark style=\"background-color:rgba(54,151,186,0.12778479422785913)\">Bottom</mark>” <mark style=\"background-color:rgba(0,127,0,0.41952857310539654)\">is</mark> <mark style=\"background-color:rgba(54,151,186,0.32049361768414014)\">to</mark> have her nephew Sylvester (Dusan Brown) do <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> <mark style=\"background-color:rgba(54,151,186,0.05138020600312278)\">spoken</mark> <mark style=\"background-color:rgba(54,151,186,0.10845310183609569)\">introduction</mark> <mark style=\"background-color:rgba(54,151,186,0.32049361768414014)\">to</mark> <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> <mark style=\"background-color:rgba(0,127,0,0.1386644989849582)\">song</mark>. <mark style=\"background-color:rgba(0,127,0,0.33435853804955773)\">it</mark>’<mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark> in this <mark style=\"background-color:rgba(0,127,0,0.28314998025538046)\">moment</mark> that <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> distance between actor, <mark style=\"background-color:rgba(0,127,0,0.15891588271182489)\">viewer</mark> <mark style=\"background-color:rgba(0,127,0,1.0)\">and</mark> role fractures: Boseman knew he was dying when he performed this <mark style=\"background-color:rgba(0,127,0,0.0790970484433229)\">monologue</mark>, <mark style=\"background-color:rgba(0,127,0,1.0)\">and</mark> some <mark style=\"background-color:rgba(54,151,186,0.1614652323711794)\">of</mark> <mark style=\"background-color:rgba(0,127,0,0.32642549477517946)\">the</mark> things he’<mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark> <mark style=\"background-color:rgba(54,151,186,0.10117968185082812)\">saying</mark> <mark style=\"background-color:rgba(0,127,0,0.3020908603287354)\">as</mark> Levee sound like questions one would ask oneself <mark style=\"background-color:rgba(54,151,186,0.1482037089286345)\">if</mark> <mark style=\"background-color:rgba(0,127,0,0.20686436170583367)\">facing</mark> one’<mark style=\"background-color:rgba(54,151,186,0.5990910388165105)\">s</mark> <mark style=\"background-color:rgba(0,127,0,0.22244059299347374)\">own</mark> mortality."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "highlight_summary(explanation_sums['review-top.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One man’s determination to keep his life completely old-school proves utterly ruinous in “Hunter Hunter,” a movie written and directed by Shawn Linden. Devon Sawa plays Joe Mersault, a trapper whose nuclear family—wife-of-saintly-patience Anne (Camille Sullivan) and eager if occasionally queasy student of the traditional ways tween daughter Renée (Summer H. Howell)—live off the land in a cabin in some deep Northern woods (the movie was largely shot in Manitoba).'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_sums['review-low.txt-test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [x] find a good pytorch/tf LSTM text classification model ~ maybe check datasets in LIME paper\n",
    "- [x] create predict_proba based on the type of the framework\n",
    "- [ ] predict on created summaries ~ automatically -> (possible: save summaries to files and then load and pass them just as normal instance) \n",
    "- [ ] refactor process to not store everything in RAM, rather put intermediate results to files\n",
    "- [x] highlighting of important words from any summary (maybe save both, str summary and Sentence type summary - from sumy)\n",
    "- [ ] extract it to separate script\n",
    "- [ ] add better logging (more logs in this version)\n",
    "- [ ] build and test quantitative experiment pipeline\n",
    "- [ ] maybe find better dataset (longer texts) for data and train another model for it\n",
    "- [ ] run quantitative experiment on all instances\n",
    "- [ ] pick several (~6) explanations for user-study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
