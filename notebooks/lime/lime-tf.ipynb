{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype of model explanation via LIME with help of extractive summary\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime(\"%Y-%m-%d_%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-4-e121ba337418>:14} CRITICAL - Logging prototype v1 with TF model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, \n",
    "    format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(filename=f'../../data/logs/v1-{now}.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "l = logging.getLogger('prototype')\n",
    "l.critical(\"Logging prototype v1 with TF model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model():\n",
    "    \"\"\"\n",
    "    Define a function that loads a model to be explained and returns its instance\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=True)\n",
    "    train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "    train_dataset.element_spec\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "    train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    VOCAB_SIZE=1000\n",
    "    encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "    encoder.adapt(train_dataset.map(lambda text, label: text))\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        encoder,\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=len(encoder.get_vocabulary()),\n",
    "            output_dim=64,\n",
    "            # Use masking to handle the variable sequence lengths\n",
    "            mask_zero=True),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "    model.load_weights('../../raw-data/lstm-model-v1')\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{dataset_builder.py:840} INFO - No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "[{dataset_info.py:362} INFO - Load dataset info from /home/tomasmizera/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n",
      "[{dataset_builder.py:323} INFO - Reusing dataset imdb_reviews (/home/tomasmizera/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
      "[{dataset_builder.py:528} INFO - Constructing tf.data.Dataset for split None, from /home/tomasmizera/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n",
      "[{<ipython-input-6-619c90ce7f16>:2} INFO - Model loaded\n"
     ]
    }
   ],
   "source": [
    "model = _load_model()\n",
    "l.info(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4518355]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"hahahahahahahahahaha this is the worst funny film I have ever seen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-a7e6831c0071>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "[{deprecation.py:317} WARNING - From <ipython-input-8-a7e6831c0071>:1: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "WARNING:tensorflow:Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n",
      "[{sequential.py:425} WARNING - Network returning invalid probability values. The last layer might not normalize predictions into probabilities (like softmax or sigmoid would).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.201608]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba([\"hahahahahahahahahaha this is the most funny film I have ever seen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers import _summarizer\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = TextRankSummarizer(Stemmer(LANGUAGE))\n",
    "summarizer.stop_words = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanator = lime_text.LimeTextExplainer(class_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-109-9e84a1d64b6b>:1} INFO - Starting an algorithm\n"
     ]
    }
   ],
   "source": [
    "l.info(\"Starting an algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to join summary\n",
    "def _get_data_from_summary(summary):\n",
    "    return ' '.join(list(map(lambda sentence: str(sentence), summary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/home/tomasmizera/school/diploma/src/data/reviews\"\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 6\n",
    "TOP_FEATURES_COUNT = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a decorator to log execusion time\n",
    "# inspired by https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        timed.calls += 1\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        timed.time_taken += (te - ts) * 1000\n",
    "        return result\n",
    "    timed.calls = 0\n",
    "    timed.time_taken = 0\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def _predict_proba(_input):\n",
    "    \"\"\"\n",
    "    Define a function that accepts array of instances and returns a probability for each class \n",
    "    _input - 1d array of instances\n",
    "    Returns 2d array of [num of instances] x [num of classes]\n",
    "    \"\"\"\n",
    "    # TODO: implement this!\n",
    "    # Mocked to simulate what LIME does in the background https://github.com/marcotcr/lime/issues/35\n",
    "#     l.debug(\"Called _predict_proba with input: \" + str(_input))\n",
    "    return np.array(5000 * [[0.4, 0.6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _explain_instance(_file, _explanator):\n",
    "    explanation = _explanator.explain_instance(_file, _predict_proba, num_features=NUM_TOP_FEATURES)\n",
    "    l.info('_explain_instance took:  %2.2f ms' % \\\n",
    "                  (_predict_proba.time_taken))\n",
    "    l.info(f'_explain_instance called {_predict_proba.calls} times')\n",
    "    _predict_proba.calls = 0\n",
    "    _predict_proba.time_taken = 0\n",
    "    \n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-114-d3ff395e95d2>:3} INFO - Took:  1.02 ms\n",
      "[{<ipython-input-114-d3ff395e95d2>:5} INFO - Called 1 times\n"
     ]
    }
   ],
   "source": [
    "expl = _explain_instance(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some examples of perturbed text**: \\[ ..., ' great hm', ' great ', ' great hm', '  hm', '  ', '  hm', '  ', '  ', '  ', '  ', 'not great ', '  ', '  hm', '  ', '  ', 'not great ', '  ', ' great ', ' great ', '  hm', '  ', ' great ', 'not  hm', ' great hm', ' great ', ' great hm', '  hm', 'not  ', '  hm', 'not  ', ' great hm', 'not great ', ' great ', '  ', '  ', '  hm', '  ', '  ', '  ', 'not  hm', 'not  ', 'not great ', 'not  hm', 'not  hm', 'not great ', 'not  hm', '  ', ' great ', '  ', '  hm', 'not  hm', '  ', '  ', ' great ', '  ', '  ', ' great ', 'not  ', 'not  hm', ' great ', 'not  ', 'not  ', 'not  hm', 'not  ', '  ', 'not great ', '  hm', ' great hm', '  hm', '  ', '  ', 'not  hm', '  hm', 'not  ', '  ', ' great hm', 'not  ', ' great hm', 'not  hm', 'not  ', 'not  hm', '  ', ' great ', '  hm', ' great ', 'not  hm', 'not  ', 'not  ', ' great hm', 'not  hm', '  hm', '  hm', ' great ', '  ', 'not great ', '  hm', 'not great ', '  ', '  ', 'not  hm', 'not great ', '  ', '  ', ' great hm', 'not  hm', 'not  hm', ' great ', ' great hm', '  ', 'not  hm', ' great ', '  hm', ' great ', 'not great ', '  ', '  ', ' great hm', ' great hm', '  ', ' great ', 'not  hm', ' great ', 'not  ', ' great ', 'not great ', ' great ', 'not  ', 'not great ', '  ', '  ', '  ', 'not  ', ' great hm', ' great hm', '  ', '  ', 'not great ', '  ', '  ', 'not  ', ' great ', ' great ', 'not great ', '  ', '  ', '  ', '  ', '  ', '  ', '  hm', '  hm', 'not  ', 'not  hm', 'not  ', '  ', ' great ', '  hm', ' great hm', '  ', '  ', '  ', ' great hm', '  ', '  ', 'not  hm', '  ', ' great hm', ' great hm', ' great ', '  ', '  ', 'not  ', '  ', 'not  ', ' great hm', 'not great ', ' great hm', 'not  hm', 'not great ', 'not  ', 'not great ', '  ', 'not great ', '  hm', 'not  ', ' great ', '  ', '  ', ' great ', '  hm', 'not  ', 'not  ', 'not  ', '  ', ' great hm', ' great hm', ' great ', ' great ', 'not great ', ' great ', '  ', 'not great ', 'not great ', '  ', 'not  ', ' great ', ' great ', '  ', '  ', ' great ', 'not  ', ' great ', '  ', '  ', ' great ', 'not  hm', 'not  ', '  ', 'not great ', '  ', '  ', '  ', 'not great ', 'not great ', '  ', '  hm', '  hm', 'not  hm', 'not great ', ' great ', 'not  ', '  ', 'not  hm', 'not great ', 'not  ', 'not great ', 'not  hm', 'not  ', 'not  hm', ' great hm', ' great ', '  hm', '  ', '  hm', '  ', 'not  ', ' great ', '  ', '  hm', 'not  hm', 'not great ', '  ', '  ', ' great ', '  ', '  ', 'not  ', 'not  ', ' great ', '  ', 'not  hm', '  ', ' great hm', '  ', '  ', '  ', ' great ', ' great ', ' great ', ' great ', ' great ', '  hm', 'not  ', ' great ', 'not  hm', '  ', ' great hm', 'not great ', '  hm', '  hm', 'not  hm', 'not  ', 'not great ', '  ', '  ', '  ', 'not  ', '  ', ' great ', '  ', 'not great ', '  ', 'not  ', '  ', '  hm', ' great ', '  ', 'not  ', ' great hm', '  ', ' great ', 'not great ', '  ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_words = expl.as_list() TODO: when predict_proba will work\n",
    "# $store -r a\n",
    "im_words = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_from_files(path_to_files):\n",
    "    \"\"\"\n",
    "    Loads all readable files in path_to_files directory\n",
    "    Returns np.array with each files content as a separate element\n",
    "    \"\"\"\n",
    "    \n",
    "    def _read_text_file(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            return reduce(lambda a, b: a + b, f.readlines())\n",
    "    \n",
    "    files_it = os.scandir(path_to_files)\n",
    "    files_contents = {}\n",
    "    \n",
    "    for file in files_it:\n",
    "        if file.is_file(): \n",
    "            files_contents[file.name] = _read_text_file(file.path)\n",
    "        \n",
    "    return files_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "npInput = input_from_files(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summarize_doc_custom(_summarizer, _instance, _explanation):\n",
    "    \"\"\"\n",
    "    Returns summary with altered weights based on explanation\n",
    "    _summarizer - summy summarizer instance\n",
    "    _instance - instance content string\n",
    "    _explanation - LIME explanation\n",
    "    \"\"\"\n",
    "    \n",
    "    def _create_weight_graph(_summarizer, _instance_doc):\n",
    "        return _summarizer.rate_sentences(_instance_doc)\n",
    "    \n",
    "    def _count_factor(_sentence, _explanation_words_weight) -> float: # returns boosting factor for sentence\n",
    "        factor = 1.0\n",
    "        exp_words = list(map(lambda x: x[0], _explanation_words_weight))\n",
    "        for word in _sentence.words:\n",
    "            if word in exp_words:\n",
    "                factor += abs(_explanation_words_weight[exp_words.index(word)][1])        \n",
    "        return factor\n",
    "    \n",
    "    parser = PlaintextParser.from_string(_instance, Tokenizer(LANGUAGE))\n",
    "    graph = _create_weight_graph(_summarizer, parser.document)\n",
    "    \n",
    "    for sentence in graph.keys():\n",
    "        factor = _count_factor(sentence, _explanation.as_list())\n",
    "        graph[sentence] = graph[sentence] * factor # TODO: normalize the factor value\n",
    "        \n",
    "    resulting_summary = _summarizer._get_best_sentences(parser.document.sentences, SENTENCES_COUNT, graph)\n",
    "    \n",
    "    return resulting_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summary_to_string(_summary):\n",
    "    if len(_summary) <= 0:\n",
    "        return \"\"\n",
    "    \n",
    "    summary_str = str(_summary[0])\n",
    "    i = 1\n",
    "    \n",
    "    while(i < len(_summary)):\n",
    "        summary_str += ' ' + str(_summary[i])\n",
    "        i += 1\n",
    "        \n",
    "    return summary_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_explanation_summaries(_instance_map, _explanator, _summarizer):\n",
    "    \"\"\"\n",
    "    Returns summaries for all input elements\n",
    "    _instance_map - map containing instance name and its content\n",
    "    _explanator - LIME explanator instance\n",
    "    _summarizer - summy summarizer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    summaries = {}\n",
    "    \n",
    "    for instance in _instance_map.keys():\n",
    "        explanation = _explain_instance(_instance_map[instance])\n",
    "        summary = _summarize_doc_custom(_summarizer, _instance_map[instance], explanation)\n",
    "        summaries[instance] = (_summary_to_string(summary), explanation.as_list())\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_summaries(_instance_map, _summarizer):\n",
    "    \"\"\"\n",
    "    Returns summaries for all input instances\n",
    "    _instance_map - map containing instance name and its content\n",
    "    _summarizer - summy summarizer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    summaries = {}\n",
    "    \n",
    "    for instance in _instance_map.keys():\n",
    "        _parser = PlaintextParser.from_string(_instance_map[instance], Tokenizer(LANGUAGE))\n",
    "        summaries[instance] = _summary_to_string(_summarizer(_parser.document, SENTENCES_COUNT))\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<mark style=\"background-color:rgba(250, 100, 0, 0.0)\">Hellloo!</mark>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "highlight_start = f'<mark style=\"background-color:rgba(250, 100, 0, {0.00})\">'\n",
    "highlight_end = '</mark>'\n",
    "text = ''.join([highlight_start, \"Hellloo!\", highlight_end])\n",
    "\n",
    "display(HTML(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_highlighted_summary(summary_pair):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-114-d3ff395e95d2>:3} INFO - Took:  1.03 ms\n",
      "[{<ipython-input-114-d3ff395e95d2>:5} INFO - Called 1 times\n",
      "[{<ipython-input-114-d3ff395e95d2>:3} INFO - Took:  1.06 ms\n",
      "[{<ipython-input-114-d3ff395e95d2>:5} INFO - Called 1 times\n",
      "[{<ipython-input-114-d3ff395e95d2>:3} INFO - Took:  1.04 ms\n",
      "[{<ipython-input-114-d3ff395e95d2>:5} INFO - Called 1 times\n",
      "[{<ipython-input-114-d3ff395e95d2>:3} INFO - Took:  1.05 ms\n",
      "[{<ipython-input-114-d3ff395e95d2>:5} INFO - Called 1 times\n",
      "[{<ipython-input-114-d3ff395e95d2>:3} INFO - Took:  1.05 ms\n",
      "[{<ipython-input-114-d3ff395e95d2>:5} INFO - Called 1 times\n",
      "[{<ipython-input-114-d3ff395e95d2>:3} INFO - Took:  1.03 ms\n",
      "[{<ipython-input-114-d3ff395e95d2>:5} INFO - Called 1 times\n",
      "[{<ipython-input-114-d3ff395e95d2>:3} INFO - Took:  1.05 ms\n",
      "[{<ipython-input-114-d3ff395e95d2>:5} INFO - Called 1 times\n",
      "[{<ipython-input-114-d3ff395e95d2>:3} INFO - Took:  1.05 ms\n",
      "[{<ipython-input-114-d3ff395e95d2>:5} INFO - Called 1 times\n",
      "[{<ipython-input-114-d3ff395e95d2>:3} INFO - Took:  1.05 ms\n",
      "[{<ipython-input-114-d3ff395e95d2>:5} INFO - Called 1 times\n"
     ]
    }
   ],
   "source": [
    "create_explanation_summaries(npInput, explanator, summarizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_simple_summaries(npInput, summarizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [ ] find a good pytorch/tf LSTM text classification model ~ maybe check datasets in LIME paper\n",
    "- [ ] create predict_proba based on the type of the framework\n",
    "- [ ] predict on created summaries ~ automatically -> (possible: save summaries to files and then load and pass them just as normal instance) \n",
    "- [ ] refactor process to not store everything in RAM, rather put intermediate results to files\n",
    "- [ ] highlighting of important words from any summary (maybe save both, str summary and Sentence type summary - from sumy)\n",
    "- [ ] extract it to separate script\n",
    "- [ ] add better logging (more logs in this version)\n",
    "- [ ] build and test quantitative experiment pipeline\n",
    "- [ ] run quantitative experiment on all instances\n",
    "- [ ] pick several (~6) explanations for user-study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
