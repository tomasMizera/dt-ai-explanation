{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype of model explanation via LIME with help of extractive summary\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import time\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = time.strftime(\"%Y-%m-%d_%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-67-d59e77e64a5c>:14} CRITICAL - Logging LIME with new TF model\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, \n",
    "    format='[{%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(filename=f'../../data/logs/v1-{now}.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "l = logging.getLogger('prototype')\n",
    "l.critical(\"Logging LIME with new TF model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model-related\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Define a function that loads a model to be explained and returns its instance\n",
    "    \"\"\"\n",
    "    \n",
    "    return keras.models.load_model(\"../../raw-data/lstm-model-sigmoid\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-62-619c90ce7f16>:2} INFO - Model loaded\n"
     ]
    }
   ],
   "source": [
    "model = _load_model()\n",
    "l.info(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0706619]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"hahahahahahahahahaha this is the most boring film I have ever seen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.948632]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba([\"hahahahahahahahahaha this is the funniest film I have ever seen\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_text\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "\n",
    "import os\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/home/tomasmizera/school/diploma/src/data/reviews\"\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 6\n",
    "TOP_FEATURES_COUNT = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = TextRankSummarizer(Stemmer(LANGUAGE))\n",
    "summarizer.stop_words = get_stop_words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanator = lime_text.LimeTextExplainer(class_names=['positive', 'negative'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-36-9e84a1d64b6b>:1} INFO - Starting an algorithm\n"
     ]
    }
   ],
   "source": [
    "l.info(\"Starting an algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to join summary\n",
    "def _get_data_from_summary(summary):\n",
    "    return ' '.join(list(map(lambda sentence: str(sentence), summary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a decorator to log execusion time\n",
    "# inspired by https://medium.com/pythonhive/python-decorator-to-measure-the-execution-time-of-methods-fa04cb6bb36d\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        timed.calls += 1\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        timed.time_taken += (te - ts) * 1000\n",
    "        return result\n",
    "    timed.calls = 0\n",
    "    timed.time_taken = 0\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def _predict_proba(_input):\n",
    "    \"\"\"\n",
    "    Define a function that accepts array of instances and returns a probability for each class \n",
    "    _input - 1d array of instances\n",
    "    Returns 2d array of [num of instances] x [num of classes]\n",
    "    \"\"\"\n",
    "    # TODO: implement this!\n",
    "    # Mocked to simulate what LIME does in the background https://github.com/marcotcr/lime/issues/35\n",
    "#     l.debug(\"Called _predict_proba with input: \" + str(_input))\n",
    "#     return np.array(5000 * [[0.4, 0.6]])\n",
    "\n",
    "    return np.array(list(zip(1-model.predict(_input), model.predict(_input))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4426748]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"worst\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _explain_instance(_file, _explanator):\n",
    "    explanation = _explanator.explain_instance(_file, _predict_proba, num_features=TOP_FEATURES_COUNT)\n",
    "    l.info('_explain_instance took:  %2.2f ms' % \\\n",
    "                  (_predict_proba.time_taken))\n",
    "    l.info(f'_explain_instance called {_predict_proba.calls} times')\n",
    "    _predict_proba.calls = 0\n",
    "    _predict_proba.time_taken = 0\n",
    "    \n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = \"This is the most ridiculous film I have ever seen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{<ipython-input-52-6b94f3b454a8>:3} INFO - _explain_instance took:  906.03 ms\n",
      "[{<ipython-input-52-6b94f3b454a8>:5} INFO - _explain_instance called 1 times\n"
     ]
    }
   ],
   "source": [
    "expl = _explain_instance(test_input, explanator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', -0.08454352874338847)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some examples of perturbed text**: \\[ ..., ' great hm', ' great ', ' great hm', '  hm', '  ', '  hm', '  ', '  ', '  ', '  ', 'not great ', '  ', '  hm', '  ', '  ', 'not great ', '  ', ' great ', ' great ', '  hm', '  ', ' great ', 'not  hm', ' great hm', ' great ', ' great hm', '  hm', 'not  ', '  hm', 'not  ', ' great hm', 'not great ', ' great ', '  ', '  ', '  hm', '  ', '  ', '  ', 'not  hm', 'not  ', 'not great ', 'not  hm', 'not  hm', 'not great ', 'not  hm', '  ', ' great ', '  ', '  hm', 'not  hm', '  ', '  ', ' great ', '  ', '  ', ' great ', 'not  ', 'not  hm', ' great ', 'not  ', 'not  ', 'not  hm', 'not  ', '  ', 'not great ', '  hm', ' great hm', '  hm', '  ', '  ', 'not  hm', '  hm', 'not  ', '  ', ' great hm', 'not  ', ' great hm', 'not  hm', 'not  ', 'not  hm', '  ', ' great ', '  hm', ' great ', 'not  hm', 'not  ', 'not  ', ' great hm', 'not  hm', '  hm', '  hm', ' great ', '  ', 'not great ', '  hm', 'not great ', '  ', '  ', 'not  hm', 'not great ', '  ', '  ', ' great hm', 'not  hm', 'not  hm', ' great ', ' great hm', '  ', 'not  hm', ' great ', '  hm', ' great ', 'not great ', '  ', '  ', ' great hm', ' great hm', '  ', ' great ', 'not  hm', ' great ', 'not  ', ' great ', 'not great ', ' great ', 'not  ', 'not great ', '  ', '  ', '  ', 'not  ', ' great hm', ' great hm', '  ', '  ', 'not great ', '  ', '  ', 'not  ', ' great ', ' great ', 'not great ', '  ', '  ', '  ', '  ', '  ', '  ', '  hm', '  hm', 'not  ', 'not  hm', 'not  ', '  ', ' great ', '  hm', ' great hm', '  ', '  ', '  ', ' great hm', '  ', '  ', 'not  hm', '  ', ' great hm', ' great hm', ' great ', '  ', '  ', 'not  ', '  ', 'not  ', ' great hm', 'not great ', ' great hm', 'not  hm', 'not great ', 'not  ', 'not great ', '  ', 'not great ', '  hm', 'not  ', ' great ', '  ', '  ', ' great ', '  hm', 'not  ', 'not  ', 'not  ', '  ', ' great hm', ' great hm', ' great ', ' great ', 'not great ', ' great ', '  ', 'not great ', 'not great ', '  ', 'not  ', ' great ', ' great ', '  ', '  ', ' great ', 'not  ', ' great ', '  ', '  ', ' great ', 'not  hm', 'not  ', '  ', 'not great ', '  ', '  ', '  ', 'not great ', 'not great ', '  ', '  hm', '  hm', 'not  hm', 'not great ', ' great ', 'not  ', '  ', 'not  hm', 'not great ', 'not  ', 'not great ', 'not  hm', 'not  ', 'not  hm', ' great hm', ' great ', '  hm', '  ', '  hm', '  ', 'not  ', ' great ', '  ', '  hm', 'not  hm', 'not great ', '  ', '  ', ' great ', '  ', '  ', 'not  ', 'not  ', ' great ', '  ', 'not  hm', '  ', ' great hm', '  ', '  ', '  ', ' great ', ' great ', ' great ', ' great ', ' great ', '  hm', 'not  ', ' great ', 'not  hm', '  ', ' great hm', 'not great ', '  hm', '  hm', 'not  hm', 'not  ', 'not great ', '  ', '  ', '  ', 'not  ', '  ', ' great ', '  ', 'not great ', '  ', 'not  ', '  ', '  hm', ' great ', '  ', 'not  ', ' great hm', '  ', ' great ', 'not great ', '  ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_words = expl.as_list() TODO: when predict_proba will work\n",
    "# $store -r a\n",
    "im_words = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_from_files(path_to_files):\n",
    "    \"\"\"\n",
    "    Loads all readable files in path_to_files directory\n",
    "    Returns np.array with each files content as a separate element\n",
    "    \"\"\"\n",
    "    \n",
    "    def _read_text_file(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            return reduce(lambda a, b: a + b, f.readlines())\n",
    "    \n",
    "    files_it = os.scandir(path_to_files)\n",
    "    files_contents = {}\n",
    "    \n",
    "    for file in files_it:\n",
    "        if file.is_file(): \n",
    "            files_contents[file.name] = _read_text_file(file.path)\n",
    "        \n",
    "    return files_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npInput = input_from_files(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summarize_doc_custom(_summarizer, _instance, _explanation):\n",
    "    \"\"\"\n",
    "    Returns summary with altered weights based on explanation\n",
    "    _summarizer - summy summarizer instance\n",
    "    _instance - instance content string\n",
    "    _explanation - LIME explanation\n",
    "    \"\"\"\n",
    "    \n",
    "    def _create_weight_graph(_summarizer, _instance_doc):\n",
    "        return _summarizer.rate_sentences(_instance_doc)\n",
    "    \n",
    "    def _count_factor(_sentence, _explanation_words_weight) -> float: # returns boosting factor for sentence\n",
    "        factor = 1.0\n",
    "        exp_words = list(map(lambda x: x[0], _explanation_words_weight))\n",
    "        for word in _sentence.words:\n",
    "            if word in exp_words:\n",
    "                factor += abs(_explanation_words_weight[exp_words.index(word)][1])        \n",
    "        return factor\n",
    "    \n",
    "    parser = PlaintextParser.from_string(_instance, Tokenizer(LANGUAGE))\n",
    "    graph = _create_weight_graph(_summarizer, parser.document)\n",
    "    \n",
    "    for sentence in graph.keys():\n",
    "        factor = _count_factor(sentence, _explanation.as_list())\n",
    "        graph[sentence] = graph[sentence] * factor # TODO: normalize the factor value\n",
    "        \n",
    "    resulting_summary = _summarizer._get_best_sentences(parser.document.sentences, SENTENCES_COUNT, graph)\n",
    "    \n",
    "    return resulting_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _summary_to_string(_summary):\n",
    "    if len(_summary) <= 0:\n",
    "        return \"\"\n",
    "    \n",
    "    summary_str = str(_summary[0])\n",
    "    i = 1\n",
    "    \n",
    "    while(i < len(_summary)):\n",
    "        summary_str += ' ' + str(_summary[i])\n",
    "        i += 1\n",
    "        \n",
    "    return summary_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_explanation_summaries(_instance_map, _explanator, _summarizer):\n",
    "    \"\"\"\n",
    "    Returns summaries for all input elements\n",
    "    _instance_map - map containing instance name and its content\n",
    "    _explanator - LIME explanator instance\n",
    "    _summarizer - summy summarizer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    summaries = {}\n",
    "    \n",
    "    for instance in _instance_map.keys():\n",
    "        explanation = _explain_instance(_instance_map[instance])\n",
    "        summary = _summarize_doc_custom(_summarizer, _instance_map[instance], explanation)\n",
    "        summaries[instance] = (_summary_to_string(summary), explanation.as_list())\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_summaries(_instance_map, _summarizer):\n",
    "    \"\"\"\n",
    "    Returns summaries for all input instances\n",
    "    _instance_map - map containing instance name and its content\n",
    "    _summarizer - summy summarizer instance\n",
    "    \"\"\"\n",
    "    \n",
    "    summaries = {}\n",
    "    \n",
    "    for instance in _instance_map.keys():\n",
    "        _parser = PlaintextParser.from_string(_instance_map[instance], Tokenizer(LANGUAGE))\n",
    "        summaries[instance] = _summary_to_string(_summarizer(_parser.document, SENTENCES_COUNT))\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "highlight_start = f'<mark style=\"background-color:rgba(250, 100, 0, {0.00})\">'\n",
    "highlight_end = '</mark>'\n",
    "text = ''.join([highlight_start, \"Hellloo!\", highlight_end])\n",
    "\n",
    "display(HTML(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_highlighted_summary(summary_pair):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_explanation_summaries(npInput, explanator, summarizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_simple_summaries(npInput, summarizer);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [x] find a good pytorch/tf LSTM text classification model ~ maybe check datasets in LIME paper\n",
    "- [x] create predict_proba based on the type of the framework\n",
    "- [ ] predict on created summaries ~ automatically -> (possible: save summaries to files and then load and pass them just as normal instance) \n",
    "- [ ] refactor process to not store everything in RAM, rather put intermediate results to files\n",
    "- [ ] highlighting of important words from any summary (maybe save both, str summary and Sentence type summary - from sumy)\n",
    "- [ ] extract it to separate script\n",
    "- [ ] add better logging (more logs in this version)\n",
    "- [ ] build and test quantitative experiment pipeline\n",
    "- [ ] maybe find better dataset (longer texts) for data and train another model for it\n",
    "- [ ] run quantitative experiment on all instances\n",
    "- [ ] pick several (~6) explanations for user-study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
